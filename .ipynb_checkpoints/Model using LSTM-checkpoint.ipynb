{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa as lr\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('features_extraction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('Data/genres_original/'+'hiphop.00032.wav',sr=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc = lr.feature.mfcc(y,sr=sr,n_mfcc=128)\n",
    "len(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addmfcc(file):\n",
    "    mfcc = lr.feature.mfcc(audio,sr=sr,n_mfcc=20)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Data/genres_original/\"\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>blues.00000.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.350088</td>\n",
       "      <td>0.088757</td>\n",
       "      <td>0.130228</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>1784.165850</td>\n",
       "      <td>129774.064525</td>\n",
       "      <td>2002.449060</td>\n",
       "      <td>85882.761315</td>\n",
       "      <td>...</td>\n",
       "      <td>52.420910</td>\n",
       "      <td>-1.690215</td>\n",
       "      <td>36.524071</td>\n",
       "      <td>-0.408979</td>\n",
       "      <td>41.597103</td>\n",
       "      <td>-2.303523</td>\n",
       "      <td>55.062923</td>\n",
       "      <td>1.221291</td>\n",
       "      <td>46.936035</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>blues.00001.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.340914</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.095948</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>1530.176679</td>\n",
       "      <td>375850.073649</td>\n",
       "      <td>2039.036516</td>\n",
       "      <td>213843.755497</td>\n",
       "      <td>...</td>\n",
       "      <td>55.356403</td>\n",
       "      <td>-0.731125</td>\n",
       "      <td>60.314529</td>\n",
       "      <td>0.295073</td>\n",
       "      <td>48.120598</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>51.106190</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>45.786282</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>blues.00002.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>0.175570</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>1552.811865</td>\n",
       "      <td>156467.643368</td>\n",
       "      <td>1747.702312</td>\n",
       "      <td>76254.192257</td>\n",
       "      <td>...</td>\n",
       "      <td>40.598766</td>\n",
       "      <td>-7.729093</td>\n",
       "      <td>47.639427</td>\n",
       "      <td>-1.816407</td>\n",
       "      <td>52.382141</td>\n",
       "      <td>-3.439720</td>\n",
       "      <td>46.639660</td>\n",
       "      <td>-2.231258</td>\n",
       "      <td>30.573025</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>blues.00003.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.404785</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>1070.106615</td>\n",
       "      <td>184355.942417</td>\n",
       "      <td>1596.412872</td>\n",
       "      <td>166441.494769</td>\n",
       "      <td>...</td>\n",
       "      <td>44.427753</td>\n",
       "      <td>-3.319597</td>\n",
       "      <td>50.206673</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>37.319130</td>\n",
       "      <td>-0.619121</td>\n",
       "      <td>37.259739</td>\n",
       "      <td>-3.407448</td>\n",
       "      <td>31.949339</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>blues.00004.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.308526</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>1835.004266</td>\n",
       "      <td>343399.939274</td>\n",
       "      <td>1748.172116</td>\n",
       "      <td>88445.209036</td>\n",
       "      <td>...</td>\n",
       "      <td>86.099236</td>\n",
       "      <td>-5.454034</td>\n",
       "      <td>75.269707</td>\n",
       "      <td>-0.916874</td>\n",
       "      <td>53.613918</td>\n",
       "      <td>-4.404827</td>\n",
       "      <td>62.910812</td>\n",
       "      <td>-11.703234</td>\n",
       "      <td>55.195160</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>rock.00095.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.352063</td>\n",
       "      <td>0.080487</td>\n",
       "      <td>0.079486</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>2008.149458</td>\n",
       "      <td>282174.689224</td>\n",
       "      <td>2106.541053</td>\n",
       "      <td>88609.749506</td>\n",
       "      <td>...</td>\n",
       "      <td>45.050526</td>\n",
       "      <td>-13.289984</td>\n",
       "      <td>41.754955</td>\n",
       "      <td>2.484145</td>\n",
       "      <td>36.778877</td>\n",
       "      <td>-6.713265</td>\n",
       "      <td>54.866825</td>\n",
       "      <td>-1.193787</td>\n",
       "      <td>49.950665</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>rock.00096.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.398687</td>\n",
       "      <td>0.075086</td>\n",
       "      <td>0.076458</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>2006.843354</td>\n",
       "      <td>182114.709510</td>\n",
       "      <td>2068.942009</td>\n",
       "      <td>82426.016726</td>\n",
       "      <td>...</td>\n",
       "      <td>33.851742</td>\n",
       "      <td>-10.848309</td>\n",
       "      <td>39.395096</td>\n",
       "      <td>1.881229</td>\n",
       "      <td>32.010040</td>\n",
       "      <td>-7.461491</td>\n",
       "      <td>39.196327</td>\n",
       "      <td>-2.795338</td>\n",
       "      <td>31.773624</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>rock.00097.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.432142</td>\n",
       "      <td>0.075268</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>2077.526598</td>\n",
       "      <td>231657.968040</td>\n",
       "      <td>1927.293153</td>\n",
       "      <td>74717.124394</td>\n",
       "      <td>...</td>\n",
       "      <td>33.597008</td>\n",
       "      <td>-12.845291</td>\n",
       "      <td>36.367264</td>\n",
       "      <td>3.440978</td>\n",
       "      <td>36.001110</td>\n",
       "      <td>-12.588070</td>\n",
       "      <td>42.502201</td>\n",
       "      <td>-2.106337</td>\n",
       "      <td>29.865515</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>rock.00098.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.362485</td>\n",
       "      <td>0.091506</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>1398.699344</td>\n",
       "      <td>240318.731073</td>\n",
       "      <td>1818.450280</td>\n",
       "      <td>109090.207161</td>\n",
       "      <td>...</td>\n",
       "      <td>46.324894</td>\n",
       "      <td>-4.416050</td>\n",
       "      <td>43.583942</td>\n",
       "      <td>1.556207</td>\n",
       "      <td>34.331261</td>\n",
       "      <td>-5.041897</td>\n",
       "      <td>47.227180</td>\n",
       "      <td>-3.590644</td>\n",
       "      <td>41.299088</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>rock.00099.wav</td>\n",
       "      <td>661794</td>\n",
       "      <td>0.358401</td>\n",
       "      <td>0.085884</td>\n",
       "      <td>0.054454</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>1609.795082</td>\n",
       "      <td>422203.216152</td>\n",
       "      <td>1797.213044</td>\n",
       "      <td>120115.632927</td>\n",
       "      <td>...</td>\n",
       "      <td>59.167755</td>\n",
       "      <td>-7.069775</td>\n",
       "      <td>73.760391</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>76.504326</td>\n",
       "      <td>-2.025783</td>\n",
       "      <td>72.189316</td>\n",
       "      <td>1.155239</td>\n",
       "      <td>49.662510</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0    blues.00000.wav  661794          0.350088         0.088757  0.130228   \n",
       "1    blues.00001.wav  661794          0.340914         0.094980  0.095948   \n",
       "2    blues.00002.wav  661794          0.363637         0.085275  0.175570   \n",
       "3    blues.00003.wav  661794          0.404785         0.093999  0.141093   \n",
       "4    blues.00004.wav  661794          0.308526         0.087841  0.091529   \n",
       "..               ...     ...               ...              ...       ...   \n",
       "994   rock.00095.wav  661794          0.352063         0.080487  0.079486   \n",
       "995   rock.00096.wav  661794          0.398687         0.075086  0.076458   \n",
       "996   rock.00097.wav  661794          0.432142         0.075268  0.081651   \n",
       "997   rock.00098.wav  661794          0.362485         0.091506  0.083860   \n",
       "998   rock.00099.wav  661794          0.358401         0.085884  0.054454   \n",
       "\n",
       "      rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0    0.002827             1784.165850          129774.064525   \n",
       "1    0.002373             1530.176679          375850.073649   \n",
       "2    0.002746             1552.811865          156467.643368   \n",
       "3    0.006346             1070.106615          184355.942417   \n",
       "4    0.002303             1835.004266          343399.939274   \n",
       "..        ...                     ...                    ...   \n",
       "994  0.000345             2008.149458          282174.689224   \n",
       "995  0.000588             2006.843354          182114.709510   \n",
       "996  0.000322             2077.526598          231657.968040   \n",
       "997  0.001211             1398.699344          240318.731073   \n",
       "998  0.000336             1609.795082          422203.216152   \n",
       "\n",
       "     spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0                2002.449060            85882.761315  ...   52.420910   \n",
       "1                2039.036516           213843.755497  ...   55.356403   \n",
       "2                1747.702312            76254.192257  ...   40.598766   \n",
       "3                1596.412872           166441.494769  ...   44.427753   \n",
       "4                1748.172116            88445.209036  ...   86.099236   \n",
       "..                       ...                     ...  ...         ...   \n",
       "994              2106.541053            88609.749506  ...   45.050526   \n",
       "995              2068.942009            82426.016726  ...   33.851742   \n",
       "996              1927.293153            74717.124394  ...   33.597008   \n",
       "997              1818.450280           109090.207161  ...   46.324894   \n",
       "998              1797.213044           120115.632927  ...   59.167755   \n",
       "\n",
       "     mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
       "0      -1.690215   36.524071    -0.408979   41.597103    -2.303523   \n",
       "1      -0.731125   60.314529     0.295073   48.120598    -0.283518   \n",
       "2      -7.729093   47.639427    -1.816407   52.382141    -3.439720   \n",
       "3      -3.319597   50.206673     0.636965   37.319130    -0.619121   \n",
       "4      -5.454034   75.269707    -0.916874   53.613918    -4.404827   \n",
       "..           ...         ...          ...         ...          ...   \n",
       "994   -13.289984   41.754955     2.484145   36.778877    -6.713265   \n",
       "995   -10.848309   39.395096     1.881229   32.010040    -7.461491   \n",
       "996   -12.845291   36.367264     3.440978   36.001110   -12.588070   \n",
       "997    -4.416050   43.583942     1.556207   34.331261    -5.041897   \n",
       "998    -7.069775   73.760391     0.028346   76.504326    -2.025783   \n",
       "\n",
       "     mfcc19_var  mfcc20_mean  mfcc20_var  label  \n",
       "0     55.062923     1.221291   46.936035  blues  \n",
       "1     51.106190     0.531217   45.786282  blues  \n",
       "2     46.639660    -2.231258   30.573025  blues  \n",
       "3     37.259739    -3.407448   31.949339  blues  \n",
       "4     62.910812   -11.703234   55.195160  blues  \n",
       "..          ...          ...         ...    ...  \n",
       "994   54.866825    -1.193787   49.950665   rock  \n",
       "995   39.196327    -2.795338   31.773624   rock  \n",
       "996   42.502201    -2.106337   29.865515   rock  \n",
       "997   47.227180    -3.590644   41.299088   rock  \n",
       "998   72.189316     1.155239   49.662510   rock  \n",
       "\n",
       "[999 rows x 60 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/features_extraction.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "    \n",
    "    a = (xx - h) // 2\n",
    "    aa = xx - a - h\n",
    "\n",
    "    b = (yy - w) // 2\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df_in):\n",
    "    features=[] \n",
    "    labels=[] \n",
    "    for index in range(0,len(df_in)):     \n",
    "      filename = df_in.iloc[index]['filename']  \n",
    "      tstart = 0 \n",
    "      tend = df_in.iloc[index]['length'] \n",
    "      species_id = df_in.iloc[index]['label']         \n",
    "      y, sr = librosa.load('Data/genres_original/'+filename,sr=22050)\n",
    "      y_cut = y[round(tstart*sr,ndigits=None)\n",
    "         :round(tend*sr, ndigits= None)]\n",
    "      data = np.array([padding(librosa.feature.mfcc(y_cut, \n",
    "         n_fft=255,hop_length=512,n_mfcc=128),130,1500)])\n",
    "      features.append(data)\n",
    "      labels.append(species_id)\n",
    "    output=np.concatenate(features,axis=0)\n",
    "    return(np.array(output), labels)\n",
    "X,y=get_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array((X-np.min(X))/(np.max(X)-np.min(X)))\n",
    "X = X/np.std(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "names_list = y\n",
    "con = LabelEncoder()\n",
    "y = con.fit_transform(names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((561, 130, 1500), (250, 130, 1500), (188, 130, 1500), 561, 250, 188)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split twice to get the validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=123)\n",
    "#Print the shapes\n",
    "X_train.shape, X_test.shape, X_val.shape, len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 128)               834048    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 837,144\n",
      "Trainable params: 837,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(130,1500)\n",
    "model = tensorflow.keras.Sequential()\n",
    "model.add(LSTM(128,input_shape=input_shape))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - ETA: 17s - loss: 3.3646 - acc: 0.0000e+ - ETA: 3s - loss: 3.2845 - acc: 0.0556     - ETA: 3s - loss: 3.2066 - acc: 0.092 - ETA: 2s - loss: 3.1613 - acc: 0.086 - ETA: 1s - loss: 3.1265 - acc: 0.091 - ETA: 1s - loss: 3.0871 - acc: 0.085 - ETA: 0s - loss: 3.0428 - acc: 0.087 - ETA: 0s - loss: 3.0282 - acc: 0.089 - 8s 814ms/step - loss: 3.0282 - acc: 0.0891 - val_loss: 2.7863 - val_acc: 0.0851\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.6239 - acc: 0.166 - ETA: 3s - loss: 2.6853 - acc: 0.138 - ETA: 3s - loss: 2.6956 - acc: 0.148 - ETA: 2s - loss: 2.7056 - acc: 0.128 - ETA: 1s - loss: 2.7194 - acc: 0.113 - ETA: 1s - loss: 2.7132 - acc: 0.104 - ETA: 0s - loss: 2.7052 - acc: 0.103 - ETA: 0s - loss: 2.7077 - acc: 0.103 - 6s 753ms/step - loss: 2.7077 - acc: 0.1034 - val_loss: 2.6828 - val_acc: 0.0851\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.5477 - acc: 0.166 - ETA: 3s - loss: 2.5863 - acc: 0.138 - ETA: 3s - loss: 2.5897 - acc: 0.148 - ETA: 2s - loss: 2.5957 - acc: 0.128 - ETA: 1s - loss: 2.6112 - acc: 0.113 - ETA: 1s - loss: 2.6045 - acc: 0.104 - ETA: 0s - loss: 2.5957 - acc: 0.103 - ETA: 0s - loss: 2.5955 - acc: 0.103 - 6s 748ms/step - loss: 2.5955 - acc: 0.1034 - val_loss: 2.5721 - val_acc: 0.0904\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.4908 - acc: 0.111 - ETA: 3s - loss: 2.5013 - acc: 0.104 - ETA: 3s - loss: 2.5038 - acc: 0.111 - ETA: 2s - loss: 2.5000 - acc: 0.128 - ETA: 1s - loss: 2.5115 - acc: 0.116 - ETA: 1s - loss: 2.5038 - acc: 0.125 - ETA: 0s - loss: 2.4961 - acc: 0.117 - ETA: 0s - loss: 2.4926 - acc: 0.115 - 6s 838ms/step - loss: 2.4926 - acc: 0.1159 - val_loss: 2.4783 - val_acc: 0.0904\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - ETA: 6s - loss: 2.4666 - acc: 0.111 - ETA: 4s - loss: 2.4426 - acc: 0.097 - ETA: 3s - loss: 2.4422 - acc: 0.069 - ETA: 2s - loss: 2.4330 - acc: 0.076 - ETA: 2s - loss: 2.4354 - acc: 0.069 - ETA: 1s - loss: 2.4257 - acc: 0.076 - ETA: 0s - loss: 2.4230 - acc: 0.069 - ETA: 0s - loss: 2.4200 - acc: 0.073 - 7s 886ms/step - loss: 2.4200 - acc: 0.0731 - val_loss: 2.4217 - val_acc: 0.0904\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.4396 - acc: 0.055 - ETA: 3s - loss: 2.4041 - acc: 0.083 - ETA: 3s - loss: 2.3994 - acc: 0.074 - ETA: 2s - loss: 2.3911 - acc: 0.114 - ETA: 2s - loss: 2.3918 - acc: 0.141 - ETA: 1s - loss: 2.3853 - acc: 0.162 - ETA: 0s - loss: 2.3849 - acc: 0.162 - ETA: 0s - loss: 2.3820 - acc: 0.162 - 7s 867ms/step - loss: 2.3820 - acc: 0.1622 - val_loss: 2.3813 - val_acc: 0.1543\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - ETA: 6s - loss: 2.4215 - acc: 0.069 - ETA: 4s - loss: 2.3968 - acc: 0.097 - ETA: 3s - loss: 2.3899 - acc: 0.125 - ETA: 2s - loss: 2.3776 - acc: 0.149 - ETA: 2s - loss: 2.3724 - acc: 0.147 - ETA: 1s - loss: 2.3654 - acc: 0.145 - ETA: 0s - loss: 2.3660 - acc: 0.134 - ETA: 0s - loss: 2.3638 - acc: 0.130 - 7s 884ms/step - loss: 2.3638 - acc: 0.1301 - val_loss: 2.3742 - val_acc: 0.1064\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.4023 - acc: 0.069 - ETA: 4s - loss: 2.3787 - acc: 0.097 - ETA: 3s - loss: 2.3725 - acc: 0.111 - ETA: 3s - loss: 2.3646 - acc: 0.125 - ETA: 2s - loss: 2.3619 - acc: 0.125 - ETA: 1s - loss: 2.3567 - acc: 0.127 - ETA: 0s - loss: 2.3568 - acc: 0.119 - ETA: 0s - loss: 2.3543 - acc: 0.115 - 7s 913ms/step - loss: 2.3543 - acc: 0.1159 - val_loss: 2.3636 - val_acc: 0.0798\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3784 - acc: 0.027 - ETA: 4s - loss: 2.3614 - acc: 0.076 - ETA: 3s - loss: 2.3550 - acc: 0.097 - ETA: 2s - loss: 2.3487 - acc: 0.107 - ETA: 2s - loss: 2.3480 - acc: 0.102 - ETA: 1s - loss: 2.3446 - acc: 0.101 - ETA: 0s - loss: 2.3450 - acc: 0.099 - ETA: 0s - loss: 2.3429 - acc: 0.099 - 7s 902ms/step - loss: 2.3429 - acc: 0.0998 - val_loss: 2.3552 - val_acc: 0.0957\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3627 - acc: 0.111 - ETA: 4s - loss: 2.3496 - acc: 0.104 - ETA: 3s - loss: 2.3438 - acc: 0.106 - ETA: 3s - loss: 2.3380 - acc: 0.121 - ETA: 2s - loss: 2.3379 - acc: 0.116 - ETA: 1s - loss: 2.3341 - acc: 0.113 - ETA: 0s - loss: 2.3342 - acc: 0.109 - ETA: 0s - loss: 2.3308 - acc: 0.117 - 8s 969ms/step - loss: 2.3308 - acc: 0.1176 - val_loss: 2.3410 - val_acc: 0.1170\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3504 - acc: 0.083 - ETA: 4s - loss: 2.3467 - acc: 0.076 - ETA: 3s - loss: 2.3427 - acc: 0.083 - ETA: 3s - loss: 2.3376 - acc: 0.093 - ETA: 2s - loss: 2.3397 - acc: 0.086 - ETA: 1s - loss: 2.3334 - acc: 0.085 - ETA: 0s - loss: 2.3297 - acc: 0.103 - ETA: 0s - loss: 2.3285 - acc: 0.114 - 7s 934ms/step - loss: 2.3285 - acc: 0.1141 - val_loss: 2.3353 - val_acc: 0.1436\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3332 - acc: 0.152 - ETA: 4s - loss: 2.3226 - acc: 0.138 - ETA: 3s - loss: 2.3192 - acc: 0.138 - ETA: 3s - loss: 2.3212 - acc: 0.125 - ETA: 2s - loss: 2.3209 - acc: 0.127 - ETA: 1s - loss: 2.3181 - acc: 0.131 - ETA: 0s - loss: 2.3164 - acc: 0.138 - ETA: 0s - loss: 2.3153 - acc: 0.146 - 7s 873ms/step - loss: 2.3153 - acc: 0.1462 - val_loss: 2.3234 - val_acc: 0.1489\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3298 - acc: 0.166 - ETA: 3s - loss: 2.3185 - acc: 0.145 - ETA: 3s - loss: 2.3114 - acc: 0.148 - ETA: 2s - loss: 2.3108 - acc: 0.145 - ETA: 2s - loss: 2.3111 - acc: 0.158 - ETA: 1s - loss: 2.3089 - acc: 0.173 - ETA: 0s - loss: 2.3096 - acc: 0.172 - ETA: 0s - loss: 2.3093 - acc: 0.174 - 7s 912ms/step - loss: 2.3093 - acc: 0.1747 - val_loss: 2.3239 - val_acc: 0.1596\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3292 - acc: 0.180 - ETA: 4s - loss: 2.3101 - acc: 0.208 - ETA: 3s - loss: 2.3047 - acc: 0.208 - ETA: 2s - loss: 2.3026 - acc: 0.215 - ETA: 2s - loss: 2.3043 - acc: 0.200 - ETA: 1s - loss: 2.3012 - acc: 0.203 - ETA: 0s - loss: 2.3029 - acc: 0.186 - ETA: 0s - loss: 2.3014 - acc: 0.180 - 7s 814ms/step - loss: 2.3014 - acc: 0.1800 - val_loss: 2.3140 - val_acc: 0.1489\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3288 - acc: 0.055 - ETA: 4s - loss: 2.3094 - acc: 0.125 - ETA: 3s - loss: 2.3020 - acc: 0.148 - ETA: 2s - loss: 2.2984 - acc: 0.170 - ETA: 1s - loss: 2.2989 - acc: 0.169 - ETA: 1s - loss: 2.2959 - acc: 0.175 - ETA: 0s - loss: 2.2985 - acc: 0.160 - ETA: 0s - loss: 2.2973 - acc: 0.155 - 6s 774ms/step - loss: 2.2973 - acc: 0.1551 - val_loss: 2.3126 - val_acc: 0.1436\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3275 - acc: 0.055 - ETA: 4s - loss: 2.3067 - acc: 0.125 - ETA: 4s - loss: 2.2995 - acc: 0.148 - ETA: 3s - loss: 2.2959 - acc: 0.170 - ETA: 2s - loss: 2.2989 - acc: 0.163 - ETA: 1s - loss: 2.2955 - acc: 0.169 - ETA: 0s - loss: 2.2973 - acc: 0.156 - ETA: 0s - loss: 2.2959 - acc: 0.156 - 7s 864ms/step - loss: 2.2959 - acc: 0.1569 - val_loss: 2.3119 - val_acc: 0.1596\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3251 - acc: 0.083 - ETA: 4s - loss: 2.3014 - acc: 0.159 - ETA: 3s - loss: 2.2960 - acc: 0.175 - ETA: 2s - loss: 2.2922 - acc: 0.191 - ETA: 2s - loss: 2.2928 - acc: 0.202 - ETA: 1s - loss: 2.2896 - acc: 0.213 - ETA: 0s - loss: 2.2916 - acc: 0.208 - ETA: 0s - loss: 2.2895 - acc: 0.203 - 6s 795ms/step - loss: 2.2895 - acc: 0.2032 - val_loss: 2.3124 - val_acc: 0.1543\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3260 - acc: 0.069 - ETA: 4s - loss: 2.3138 - acc: 0.131 - ETA: 3s - loss: 2.3018 - acc: 0.157 - ETA: 2s - loss: 2.2965 - acc: 0.177 - ETA: 2s - loss: 2.2965 - acc: 0.191 - ETA: 1s - loss: 2.2943 - acc: 0.201 - ETA: 0s - loss: 2.2962 - acc: 0.196 - ETA: 0s - loss: 2.2961 - acc: 0.192 - 7s 952ms/step - loss: 2.2961 - acc: 0.1925 - val_loss: 2.3204 - val_acc: 0.1330\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3250 - acc: 0.083 - ETA: 4s - loss: 2.3073 - acc: 0.159 - ETA: 4s - loss: 2.2989 - acc: 0.171 - ETA: 3s - loss: 2.2941 - acc: 0.187 - ETA: 2s - loss: 2.2938 - acc: 0.200 - ETA: 1s - loss: 2.2899 - acc: 0.206 - ETA: 0s - loss: 2.2906 - acc: 0.200 - ETA: 0s - loss: 2.2885 - acc: 0.196 - 7s 936ms/step - loss: 2.2885 - acc: 0.1961 - val_loss: 2.2997 - val_acc: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3149 - acc: 0.083 - ETA: 4s - loss: 2.2951 - acc: 0.152 - ETA: 3s - loss: 2.2874 - acc: 0.166 - ETA: 2s - loss: 2.2855 - acc: 0.184 - ETA: 2s - loss: 2.2854 - acc: 0.194 - ETA: 1s - loss: 2.2823 - acc: 0.206 - ETA: 0s - loss: 2.2844 - acc: 0.200 - ETA: 0s - loss: 2.2842 - acc: 0.196 - 7s 944ms/step - loss: 2.2842 - acc: 0.1961 - val_loss: 2.3058 - val_acc: 0.1436\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - ETA: 6s - loss: 2.3156 - acc: 0.083 - ETA: 5s - loss: 2.2907 - acc: 0.159 - ETA: 4s - loss: 2.2851 - acc: 0.171 - ETA: 3s - loss: 2.2829 - acc: 0.191 - ETA: 2s - loss: 2.2833 - acc: 0.200 - ETA: 1s - loss: 2.2804 - acc: 0.208 - ETA: 0s - loss: 2.2817 - acc: 0.202 - ETA: 0s - loss: 2.2801 - acc: 0.197 - 7s 936ms/step - loss: 2.2801 - acc: 0.1979 - val_loss: 2.2952 - val_acc: 0.1543\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3104 - acc: 0.083 - ETA: 5s - loss: 2.2874 - acc: 0.159 - ETA: 4s - loss: 2.2834 - acc: 0.171 - ETA: 3s - loss: 2.2828 - acc: 0.187 - ETA: 2s - loss: 2.2846 - acc: 0.200 - ETA: 1s - loss: 2.2831 - acc: 0.208 - ETA: 0s - loss: 2.2853 - acc: 0.202 - ETA: 0s - loss: 2.2848 - acc: 0.197 - 8s 975ms/step - loss: 2.2848 - acc: 0.1979 - val_loss: 2.2982 - val_acc: 0.1436\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3102 - acc: 0.083 - ETA: 3s - loss: 2.2890 - acc: 0.152 - ETA: 3s - loss: 2.2813 - acc: 0.166 - ETA: 2s - loss: 2.2797 - acc: 0.187 - ETA: 2s - loss: 2.2806 - acc: 0.200 - ETA: 1s - loss: 2.2776 - acc: 0.210 - ETA: 0s - loss: 2.2798 - acc: 0.204 - ETA: 0s - loss: 2.2802 - acc: 0.199 - 7s 858ms/step - loss: 2.2802 - acc: 0.1996 - val_loss: 2.3115 - val_acc: 0.1330\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3146 - acc: 0.083 - ETA: 3s - loss: 2.3006 - acc: 0.152 - ETA: 3s - loss: 2.2977 - acc: 0.166 - ETA: 2s - loss: 2.2945 - acc: 0.180 - ETA: 1s - loss: 2.2923 - acc: 0.194 - ETA: 1s - loss: 2.2867 - acc: 0.206 - ETA: 0s - loss: 2.2863 - acc: 0.200 - ETA: 0s - loss: 2.2859 - acc: 0.196 - 6s 811ms/step - loss: 2.2859 - acc: 0.1961 - val_loss: 2.3193 - val_acc: 0.1170\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - ETA: 6s - loss: 2.3344 - acc: 0.055 - ETA: 5s - loss: 2.3253 - acc: 0.090 - ETA: 4s - loss: 2.3123 - acc: 0.111 - ETA: 3s - loss: 2.3015 - acc: 0.142 - ETA: 2s - loss: 2.2980 - acc: 0.163 - ETA: 1s - loss: 2.2951 - acc: 0.173 - ETA: 0s - loss: 2.2959 - acc: 0.170 - ETA: 0s - loss: 2.2954 - acc: 0.169 - 8s 976ms/step - loss: 2.2954 - acc: 0.1693 - val_loss: 2.3200 - val_acc: 0.1064\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3268 - acc: 0.069 - ETA: 4s - loss: 2.3135 - acc: 0.118 - ETA: 3s - loss: 2.3106 - acc: 0.125 - ETA: 2s - loss: 2.3079 - acc: 0.135 - ETA: 2s - loss: 2.3085 - acc: 0.141 - ETA: 1s - loss: 2.3062 - acc: 0.145 - ETA: 0s - loss: 2.3061 - acc: 0.142 - ETA: 0s - loss: 2.3047 - acc: 0.144 - 7s 930ms/step - loss: 2.3047 - acc: 0.1444 - val_loss: 2.3217 - val_acc: 0.1011\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3303 - acc: 0.055 - ETA: 4s - loss: 2.3164 - acc: 0.104 - ETA: 4s - loss: 2.3124 - acc: 0.115 - ETA: 3s - loss: 2.3086 - acc: 0.128 - ETA: 2s - loss: 2.3088 - acc: 0.136 - ETA: 1s - loss: 2.3057 - acc: 0.143 - ETA: 0s - loss: 2.3051 - acc: 0.142 - ETA: 0s - loss: 2.3034 - acc: 0.144 - 7s 918ms/step - loss: 2.3034 - acc: 0.1444 - val_loss: 2.3190 - val_acc: 0.1064\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3238 - acc: 0.069 - ETA: 4s - loss: 2.3118 - acc: 0.118 - ETA: 3s - loss: 2.3095 - acc: 0.125 - ETA: 2s - loss: 2.3050 - acc: 0.138 - ETA: 2s - loss: 2.3047 - acc: 0.150 - ETA: 1s - loss: 2.3020 - acc: 0.155 - ETA: 0s - loss: 2.3020 - acc: 0.152 - ETA: 0s - loss: 2.3003 - acc: 0.153 - 7s 848ms/step - loss: 2.3003 - acc: 0.1533 - val_loss: 2.3176 - val_acc: 0.1064\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3226 - acc: 0.069 - ETA: 4s - loss: 2.3113 - acc: 0.118 - ETA: 4s - loss: 2.3091 - acc: 0.125 - ETA: 3s - loss: 2.3032 - acc: 0.142 - ETA: 2s - loss: 2.3026 - acc: 0.158 - ETA: 1s - loss: 2.2995 - acc: 0.169 - ETA: 0s - loss: 2.2994 - acc: 0.166 - ETA: 0s - loss: 2.2977 - acc: 0.165 - 7s 929ms/step - loss: 2.2977 - acc: 0.1658 - val_loss: 2.3099 - val_acc: 0.1330\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3101 - acc: 0.083 - ETA: 4s - loss: 2.2926 - acc: 0.159 - ETA: 3s - loss: 2.2845 - acc: 0.171 - ETA: 2s - loss: 2.2785 - acc: 0.187 - ETA: 2s - loss: 2.2778 - acc: 0.188 - ETA: 1s - loss: 2.2750 - acc: 0.187 - ETA: 0s - loss: 2.2757 - acc: 0.186 - ETA: 0s - loss: 2.2734 - acc: 0.185 - 7s 837ms/step - loss: 2.2734 - acc: 0.1854 - val_loss: 2.2862 - val_acc: 0.1596\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.2992 - acc: 0.083 - ETA: 4s - loss: 2.2774 - acc: 0.152 - ETA: 3s - loss: 2.2730 - acc: 0.166 - ETA: 3s - loss: 2.2715 - acc: 0.184 - ETA: 2s - loss: 2.2735 - acc: 0.197 - ETA: 1s - loss: 2.2729 - acc: 0.206 - ETA: 0s - loss: 2.2756 - acc: 0.200 - ETA: 0s - loss: 2.2761 - acc: 0.196 - 7s 901ms/step - loss: 2.2761 - acc: 0.1961 - val_loss: 2.3097 - val_acc: 0.1223\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3108 - acc: 0.083 - ETA: 4s - loss: 2.2941 - acc: 0.152 - ETA: 4s - loss: 2.2894 - acc: 0.175 - ETA: 3s - loss: 2.2846 - acc: 0.194 - ETA: 2s - loss: 2.2825 - acc: 0.205 - ETA: 1s - loss: 2.2775 - acc: 0.215 - ETA: 0s - loss: 2.2775 - acc: 0.212 - ETA: 0s - loss: 2.2752 - acc: 0.206 - 7s 902ms/step - loss: 2.2752 - acc: 0.2068 - val_loss: 2.2933 - val_acc: 0.1543\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - ETA: 5s - loss: 2.3081 - acc: 0.069 - ETA: 5s - loss: 2.2880 - acc: 0.138 - ETA: 4s - loss: 2.2763 - acc: 0.157 - ETA: 3s - loss: 2.2725 - acc: 0.177 - ETA: 2s - loss: 2.2718 - acc: 0.191 - ETA: 1s - loss: 2.2694 - acc: 0.203 - ETA: 0s - loss: 2.2718 - acc: 0.198 - ETA: 0s - loss: 2.2720 - acc: 0.194 - 7s 954ms/step - loss: 2.2720 - acc: 0.1943 - val_loss: 2.2985 - val_acc: 0.1330\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - ETA: 7s - loss: 2.3045 - acc: 0.083 - ETA: 5s - loss: 2.2852 - acc: 0.159 - ETA: 4s - loss: 2.2811 - acc: 0.171 - ETA: 3s - loss: 2.2772 - acc: 0.187 - ETA: 2s - loss: 2.2753 - acc: 0.200 - ETA: 1s - loss: 2.2705 - acc: 0.210 - ETA: 0s - loss: 2.2710 - acc: 0.206 - ETA: 0s - loss: 2.2684 - acc: 0.201 - 8s 1s/step - loss: 2.2684 - acc: 0.2014 - val_loss: 2.2875 - val_acc: 0.1543\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - ETA: 4s - loss: 2.3058 - acc: 0.069 - ETA: 4s - loss: 2.2845 - acc: 0.138 - ETA: 4s - loss: 2.2733 - acc: 0.157 - ETA: 3s - loss: 2.2698 - acc: 0.177 - ETA: 2s - loss: 2.2685 - acc: 0.191 - ETA: 1s - loss: 2.2658 - acc: 0.206 - ETA: 0s - loss: 2.2678 - acc: 0.200 - ETA: 0s - loss: 2.2677 - acc: 0.196 - 8s 1s/step - loss: 2.2677 - acc: 0.1961 - val_loss: 2.2889 - val_acc: 0.1436\n",
      "Epoch 36/50\n",
      "5/8 [=================>............] - ETA: 5s - loss: 2.2996 - acc: 0.083 - ETA: 4s - loss: 2.2707 - acc: 0.159 - ETA: 3s - loss: 2.2658 - acc: 0.171 - ETA: 3s - loss: 2.2634 - acc: 0.187 - ETA: 2s - loss: 2.2627 - acc: 0.1972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-362-fbb1170ad961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n\u001b[1;32m----> 2\u001b[1;33m                     validation_data=(X_val, y_val), shuffle=False)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=72, \n",
    "                    validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "for name in df.iloc[:,0]:\n",
    "    audio_file = glob(data_dir + name)\n",
    "    audio,sfreq = lr.load(audio_file[0],sr=sr)\n",
    "    mfccs.append(addmfcc(audio))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-211.48465 , -208.94481 , -193.90889 , ..., -109.999146,\n",
       "        -86.84641 ,  -79.03764 ], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mfccs = {}\n",
    "for i in range(20):\n",
    "    new_mfccs['mfcc'+str(i+1)] = [mfccs[j][i] for j in range(len(mfccs))]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
